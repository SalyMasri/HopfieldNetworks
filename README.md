# Hopfield Networks & Associative Memory

## ğŸ“Œ Project Overview
This project explores **Hopfield networks** as **autoassociative memory systems**, analyzing their capabilities for **pattern completion, noise resistance, and energy-based convergence**. The study also investigates storage limitations and the differences between **synchronous (Little model) and asynchronous (sequential) updates**.

## ğŸ› ï¸ Technologies & Tools Used
- **Programming Language**: Python
- **Libraries**: NumPy, Matplotlib, Google Colab
- **Training Rule**: Hebbian Learning
- **Update Methods**: Synchronous vs. Asynchronous (Random Order)
- **Evaluation Metrics**: Energy Function Analysis, Convergence Rate, Pattern Recall Accuracy

## ğŸ” Key Features
### **ğŸ§  Pattern Recall & Noise Resistance**
- **Tested recall with noisy inputs** and analyzed attractor stability.
- Measured **network robustness** by introducing **1-bit to 5-bit distortions**.
- **Spurious Attractors**: Investigated unexpected network states beyond stored patterns.

### **ğŸ“‰ Energy Function Analysis**
- Tracked **energy evolution** to validate network convergence.
- Compared energy behavior under **random weight matrices vs. symmetric weight matrices**.

### **ğŸ”„ Storage Capacity & Scaling**
- **Tested storage limits** by increasing the number of patterns.
- Observed **degradation in recall accuracy** beyond optimal capacity (~0.138N).
- **Sparse Encoding**: Evaluated how sparsity influences memory efficiency.

## ğŸ“Š Results
âœ”ï¸ **Hopfield networks can restore patterns with up to ~30% noise** but fail beyond 50%.  
âœ”ï¸ **Sequential updates improve convergence stability** but may lead to **pathological attractors**.  
âœ”ï¸ **Random weight matrices fail to converge**, whereas **symmetric matrices ensure stability**.  
âœ”ï¸ **Pattern recall accuracy decreases sharply beyond storage capacity limits**.  
âœ”ï¸ **Sparse encoding improves recall efficiency**, reducing interference between stored patterns.


